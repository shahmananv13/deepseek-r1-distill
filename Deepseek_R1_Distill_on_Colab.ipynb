{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjCBYrEzNIZT"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn -q\n",
        "!pip install vllm -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and run the model:\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Start vllm server in the background\n",
        "vllm_process = subprocess.Popen([\n",
        "    'vllm',\n",
        "    'serve',  # Subcommand must follow vllm\n",
        "    'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
        "    '--trust-remote-code',\n",
        "    '--dtype', 'half',\n",
        "    '--max-model-len', '16384', # This is max token input and output that you send and retrieve\n",
        "    '--enable-chunked-prefill', 'true',\n",
        "    '--tensor-parallel-size', '1'\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)"
      ],
      "metadata": {
        "id": "9PFt1ovONVMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def check_vllm_status():\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8000/health\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"vllm server is running\")\n",
        "            return True\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"vllm server is not running\")\n",
        "        return False\n",
        "\n",
        "try:\n",
        "    # Monitor the process\n",
        "    while True:\n",
        "        if check_vllm_status() == True:\n",
        "            print(\"The vllm server is ready to serve.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"The vllm server has stopped.\")\n",
        "            stdout, stderr = vllm_process.communicate(timeout = 10)\n",
        "            print(f\"STDOUT: {stdout.decode('utf-8')}\")\n",
        "            print(f\"STDERR: {stderr.decode('utf-8')}\")\n",
        "            break\n",
        "        time.sleep(5)  # Check every second\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopping the check of vllm...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCBhb_0WOR5k",
        "outputId": "bee4732c-d62c-43cf-f9d9-079b9762192e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vllm server is running\n",
            "The vllm server is ready to serve.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from fastapi.responses import StreamingResponse\n",
        "import requests\n",
        "\n",
        "# Request schema for input\n",
        "class QuestionRequest(BaseModel):\n",
        "    question: str\n",
        "    model: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Default model\n",
        "\n",
        "\n",
        "def ask_model(question: str, model: str):\n",
        "    \"\"\"\n",
        "    Sends a request to the model server and fetches a response.\n",
        "    \"\"\"\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"  # Adjust the URL if different\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": question\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Raise exception for HTTP errors\n",
        "    return response.json()\n",
        "\n",
        "# Usage:\n",
        "result = ask_model(\"What is the capital of France?\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "print(json.dumps(result, indent=2))\n",
        "\n",
        "def stream_llm_response(question:str, model:str):\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
        "        \"stream\": True  # ðŸ”¥ Enable streaming\n",
        "    }\n",
        "\n",
        "    with requests.post(url, headers=headers, json=data, stream=True) as response:\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                # OpenAI-style streaming responses are prefixed with \"data: \"\n",
        "                decoded_line = line.decode(\"utf-8\").replace(\"data: \", \"\")\n",
        "                yield decoded_line + \"\\n\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPYdSE-ZOftl",
        "outputId": "07317640-bf11-4cb4-c8c6-424e22149443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-6c1dc12a934d4d16a3d8987a3bc325fa\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1738242381,\n",
            "  \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"<think>\\nThe user is asking about the capital of France, so I know it's Paris. I should confirm that without marketing it seems appropriate.\\n\\nI need to present the information clearly and in a straightforward manner.\\n\\nIt's essential to keep the response accurate and simple, avoiding any unnecessary details.\\n\\nI'll make sure the response is correct and helpful.\\n</think>\\n\\nThe capital of France is Paris.\",\n",
            "        \"tool_calls\": []\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 10,\n",
            "    \"total_tokens\": 89,\n",
            "    \"completion_tokens\": 79,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import getpass\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get('/')\n",
        "async def root():\n",
        "    return {'hello': 'world'}\n",
        "@app.post(\"/api/v1/generate-response\")\n",
        "def generate_response(request: QuestionRequest):\n",
        "    \"\"\"\n",
        "    API endpoint to generate a response from the model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = ask_model(request.question, request.model)\n",
        "        return {\"response\": response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/api/v1/generate-response-stream\")\n",
        "def stream_response(request:QuestionRequest):\n",
        "  try:\n",
        "    response = stream_llm_response(request.question, request.model)\n",
        "    return StreamingResponse(response, media_type=\"text/event-stream\")\n",
        "  except Exception as e:\n",
        "    raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "Yyf8V79_QNae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken {your_token}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNR6KrtTQyYM",
        "outputId": "8b0f8764-19fc-4cc1-801c-4ba293245743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "port = 8081\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3ckVyueQ0su",
        "outputId": "e4bafa7d-6643-40ce-a2ae-2d0fd9443c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://66e7-35-197-155-192.ngrok-free.app\" -> \"http://127.0.0.1:8081\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=port)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9vmW42RRL8m",
        "outputId": "06ca2b0b-98bb-4c9d-9965-ae9dd34d9733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [3591]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8081 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     34.106.71.239:0 - \"POST /api/v1/generate-response-stream HTTP/1.1\" 200 OK\n",
            "INFO:     34.106.71.239:0 - \"POST /api/v1/generate-response HTTP/1.1\" 200 OK\n",
            "INFO:     34.106.71.239:0 - \"POST /api/v1/generate-response HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [3591]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nl9nw7FmRp5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}